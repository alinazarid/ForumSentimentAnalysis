{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_direct = '../training_data/train.pkl'\n",
    "test_direct = '../testing_data/test.pkl'\n",
    "d2v_direct = \"../Doc2Vec/d2v.model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter as ctr\n",
    "from bs4 import BeautifulSoup\n",
    "import collections as c\n",
    "from nltk.tokenize import word_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(train_direct)\n",
    "train['text'] = train.msg.map(lambda x: x.get_text())  \n",
    "\n",
    "test = pd.read_pickle(test_direct)\n",
    "test = test.dropna(subset=['msgID'])\n",
    "test['text'] = test.msg.map(lambda x: x.get_text())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = train.dropna(subset=['label'])\n",
    "test_labeled = test.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dov2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03047852, -0.05406708,  0.00684914, -0.09190172,  0.10686632,\n",
       "        0.0109821 , -0.02377012, -0.08077868,  0.0688705 ,  0.104604  ,\n",
       "        0.10747099,  0.00182713, -0.03931361, -0.05217262, -0.06938926,\n",
       "        0.00916352,  0.00571766, -0.03961769,  0.01551475, -0.02399089],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "#Loading trained model\n",
    "vec_model= Doc2Vec.load(d2v_direct)\n",
    "\n",
    "#Obtaining vectors and storing them in training df\n",
    "train_labeled['vecs'] = train_labeled['text'].map(lambda x: vec_model.infer_vector(word_tokenize(x.lower())))\n",
    "#Doing the same for test df\n",
    "test_labeled['vecs'] = test_labeled['text'].map(lambda x: vec_model.infer_vector(word_tokenize(x.lower())))\n",
    "train_labeled['vecs'].iloc[0] # An example how the code would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained vectors\n",
    "vectors = vec_model.docvecs    # Each post is turned into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix discrepancy issue between training data ranks and testing data ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume: Mod Squad --> Mod\n",
    "#         Frequent Visitor --> Visitor\n",
    "#         Post Mod --> Mod\n",
    "train_labeled.loc[train_labeled['rank'] == 'Mod Squad', 'rank'] = 'Mod' \n",
    "train_labeled.loc[train_labeled['rank'] == 'Post Mod', 'rank'] = 'Mod'\n",
    "train_labeled.loc[train_labeled['rank'] == 'Frequent Visitor', 'rank'] = 'Visitor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining dataframes for NN's\n",
    "df_NN_train = train_labeled.drop(['msgID', 'msg', 'authorID', 'fine_grained', 'affiliation', 'text'], 1)\n",
    "df_NN_test =  test_labeled.drop(['msgID', 'msg', 'authorID', 'fine_grained', 'affiliation', 'text'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = max(c.Counter(df_NN_test['label']).values())/(len(df_NN_test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(df_NN_train)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amber', 'crisis', 'green', 'red'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(df_NN_train['label'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with embedding (FS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add embedings as inputs for NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1188, 20), (1188, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "le.fit(df_NN_train.label)\n",
    "y = le.transform(df_NN_train.label).reshape(-1, 1)    # Your predictions are labeles\n",
    "ohe.fit(y)\n",
    "y_train = ohe.transform(y)\n",
    "\n",
    "#indexing removed\n",
    "X_train = np.array([x for x in df_NN_train['vecs']])    # Using only vector of words as inputs!\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((400, 20), (400, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(df_NN_test.label)\n",
    "y_test = le.transform(df_NN_test.label).reshape(-1, 1)    # Your predictions are labeles\n",
    "ohe.fit(y_test)\n",
    "y_test = ohe.transform(y_test)\n",
    "\n",
    "#indexing removed\n",
    "X_test = np.array([x for x in df_NN_test.vecs])    # Using only vector of words as inputs!\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NN model with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training. We can change the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alinazarid/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/alinazarid/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0834586a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a model using Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#Input layer\n",
    "model.add(Dense(units=256, kernel_initializer=initializers.Constant(1),bias_initializer='zeros', activation='sigmoid', input_dim=X_train.shape[1],kernel_regularizer=regularizers.l2(0.01)))\n",
    "#model.add(Dense(units=256, activation='sigmoid', input_dim=X_train.shape[1],kernel_regularizer=regularizers.l2(0.01)))\n",
    "#Adding a second layer (it becomes deep learning)\n",
    "#model.add(Dense(units=64, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "#Output layer\n",
    "model.add(Dense(units=len(labels), activation='softmax'))\n",
    "\n",
    "#Similar to KL diversion\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.2790551805496215\n",
      "Test accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with embedding and sentiments (FS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Sent scores to NN inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1188, 24), (400, 24))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the sentiment scores\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "train_labeled['sent_score'] = train_labeled['text'].map(analyser.polarity_scores)\n",
    "df_NN_train['sent_score'] = train_labeled['sent_score'].map(lambda x: list(x.values()))\n",
    "X = np.array([x for x in df_NN_train['sent_score']])    # Using only vector of words as inputs!\n",
    "X_train = np.concatenate((X_train,X), axis = 1)\n",
    "\n",
    "# same for testing data\n",
    "test_labeled['sent_score'] = test_labeled['text'].map(analyser.polarity_scores)\n",
    "df_NN_test['sent_score'] = test_labeled['sent_score'].map(lambda x: list(x.values()))\n",
    "X = np.array([x for x in df_NN_test['sent_score']])    # Using only vector of words as inputs!\n",
    "X_test = np.concatenate((X_test,X), axis = 1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.168, 'neu': 0.714, 'pos': 0.118, 'compound': -0.6516},\n",
       " [0.168, 0.714, 0.118, -0.6516])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled['sent_score'].iloc[0], df_NN_train['sent_score'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NN model with embedding and sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff07eaa9f60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a model using Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#Input layer\n",
    "model.add(Dense(units=256, kernel_initializer=initializers.Constant(1),bias_initializer='zeros', activation='sigmoid', input_dim=X_train.shape[1],kernel_regularizer=regularizers.l2(0.01)))\n",
    "#model.add(Dense(units=256, activation='sigmoid', input_dim=X_train.shape[1],kernel_regularizer=regularizers.l2(0.01)))\n",
    "#Adding a second layer (it becomes deep learning)\n",
    "#model.add(Dense(units=64, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "#Output layer\n",
    "model.add(Dense(units=len(labels), activation='softmax'))\n",
    "\n",
    "#Similar to KL diversion\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.1948605251312256\n",
      "Test accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with embedding, sentiments, ranks (FS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ranks to NN inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/bin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1188, 35), (400, 35))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Add rank as another feature\n",
    "# \n",
    "X = df_NN_train[['rank']]\n",
    "X['rank'] = le.fit_transform(X['rank'])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "X_train = np.concatenate((X_train,X), axis = 1)\n",
    "\n",
    "#\n",
    "# Add rank as another feature\n",
    "# \n",
    "X = df_NN_test[['rank']]\n",
    "X['rank'] = le.fit_transform(X['rank'])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "X_test = np.concatenate((X_test,X), axis = 1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NN model with embedding, sentiments, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff07dde7518>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a model using Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#Input layer\n",
    "model.add(Dense(units=256, kernel_initializer=initializers.Constant(1),bias_initializer='zeros', activation='sigmoid', input_dim=X_train.shape[1],kernel_regularizer=regularizers.l2(0.01)))\n",
    "#model.add(Dense(units=256, activation='sigmoid', input_dim=X_train.shape[1],kernel_regularizer=regularizers.l2(0.01)))\n",
    "#Adding a second layer (it becomes deep learning)\n",
    "#model.add(Dense(units=64, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "#Output layer\n",
    "model.add(Dense(units=len(labels), activation='softmax'))\n",
    "\n",
    "#Similar to KL diversion\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.2541744899749756\n",
      "Test accuracy: 0.555\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
