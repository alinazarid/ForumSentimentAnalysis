{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "Ref: [Tutorial#1](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)\n",
    "[Tutorial#2](http://adventuresinmachinelearning.com/keras-lstm-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_direct = '../training_data/train.pkl'\n",
    "test_direct = '../testing_data/test.pkl'\n",
    "model01_direct = \"./models/RNN_model_v1._.hdf5\"\n",
    "model02_direct = \"./models/RNN_model_v2.2.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as c\n",
    "from sklearn import preprocessing\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(train_direct)\n",
    "train['text'] = train.msg.map(lambda x: x.get_text())  \n",
    "\n",
    "test = pd.read_pickle(test_direct)\n",
    "test = test.dropna(subset=['msgID'])\n",
    "test['text'] = test.msg.map(lambda x: x.get_text())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get an entire vocabulary of all words in training and testing posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9217168"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokens'] = train['text'].map(word_tokenize)\n",
    "test['tokens'] = test['text'].map(word_tokenize)\n",
    "posts_list = list(train['tokens'])\n",
    "posts_list.extend(list(test['tokens']))\n",
    "vocab =list()\n",
    "for p  in posts_list: vocab.extend(p)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove the posts without label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the labeled data\n",
    "train_labeled = train.dropna(subset=['label'])\n",
    "test_labeled = test.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    # a list of all words! for us will be all posts regardless of their label\n",
    "    counter = c.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "def file_to_word_ids(data, word_to_id):\n",
    "    # data:  this will be a labeled post\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/bin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1616, 776)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id = build_vocab(vocab)\n",
    "train_labeled['integers'] = train_labeled['tokens'].map(lambda l: file_to_word_ids(l, word_to_id))\n",
    "test_labeled['integers'] = test_labeled['tokens'].map(lambda l: file_to_word_ids(l, word_to_id))\n",
    "max(len(l) for l in train_labeled['integers']),max(len(l) for l in test_labeled['integers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1188, 770), (400, 770))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hence we chose 770 words to consider in each post\n",
    "max_post_len = 770\n",
    "X_train = sequence.pad_sequences(train_labeled['integers'], maxlen = max_post_len)\n",
    "X_test = sequence.pad_sequences(test_labeled['integers'], maxlen = max_post_len)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/bin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1188, 4), (400, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "le.fit(train_labeled['label'])\n",
    "y = le.transform(train_labeled['label']).reshape(-1, 1)    # Your predictions are labeles\n",
    "ohe.fit(y)\n",
    "y_train = ohe.transform(y)\n",
    "\n",
    "le.fit(test_labeled['label'])\n",
    "y = le.transform(test_labeled['label']).reshape(-1, 1)    # Your predictions are labeles\n",
    "ohe.fit(y)\n",
    "y_test = ohe.transform(y)\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RNN model version 1\n",
    "Single LSTM layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation, adam optimizer!\n",
    "embedding_len = 50    # Length of embedding for each word\n",
    "top_words_len = len(vocab)    \n",
    "batch_len = 8    # number of posts used to space out weight updates\n",
    "num_epoch = 3\n",
    "LSTM_hidden_size = 100\n",
    "use_dropout=True\n",
    "drop = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 770, 50)           6762700   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 6,823,504\n",
      "Trainable params: 6,823,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1188 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "1188/1188 [==============================] - 78s 66ms/step - loss: nan - acc: 0.3140 - val_loss: nan - val_acc: 0.2350\n",
      "Epoch 2/3\n",
      "1188/1188 [==============================] - 77s 65ms/step - loss: nan - acc: 0.2492 - val_loss: nan - val_acc: 0.2350\n",
      "Epoch 3/3\n",
      "1188/1188 [==============================] - 77s 65ms/step - loss: nan - acc: 0.2492 - val_loss: nan - val_acc: 0.2350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fd96f66d8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words_len, embedding_len, input_length=max_post_len))\n",
    "model.add(LSTM(LSTM_hidden_size))\n",
    "if use_dropout: model.add(Dropout(drop))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          epochs=num_epoch, batch_size=batch_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model01_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model01_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.50%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RNN model version 2\n",
    "Add a layer of CNN to version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation, adam optimizer!\n",
    "embedding_len = 50    # Length of embedding for each word\n",
    "top_words_len = len(vocab)    \n",
    "batch_len = 8    # number of posts used to space out weight updates\n",
    "num_epoch = 3\n",
    "LSTM_hidden_size = 100\n",
    "use_dropout=True\n",
    "drop = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 770, 50)           6762700   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 770, 32)           4832      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 385, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 6,821,136\n",
      "Trainable params: 6,821,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1188 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "1188/1188 [==============================] - 44s 37ms/step - loss: 0.9956 - acc: 0.6027 - val_loss: 1.0307 - val_acc: 0.5450\n",
      "Epoch 2/3\n",
      "1188/1188 [==============================] - 41s 34ms/step - loss: 0.6893 - acc: 0.7231 - val_loss: 0.9938 - val_acc: 0.6125\n",
      "Epoch 3/3\n",
      "1188/1188 [==============================] - 40s 34ms/step - loss: 0.4707 - acc: 0.7938 - val_loss: 1.0117 - val_acc: 0.6325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ed2abdb38>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words_len, embedding_len,input_length=max_post_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(LSTM_hidden_size))\n",
    "if use_dropout: model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1],\n",
    "kernel_initializer = initializers.RandomNormal(mean=0.2, stddev=0.4, seed=None),\n",
    "                bias_initializer='zeros',\n",
    "                activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          epochs=num_epoch, batch_size=batch_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model02_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model02_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.00%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
